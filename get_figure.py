import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set the seaborn style
sns.set(style="whitegrid")

# The actual value of the cosine similarity
cosine_values = [0.5, 0.6, 0.7, 0.8, 0.9, 0.93, 0.96, 0.99]

# Average data of dimension 16
# complexity:
# recall_16 = [0.913, 0.879, 0.829, 0.753, 0.619, 0.551, 0.444, 0.253]
# precision_16 = [0.690, 0.699, 0.713, 0.734, 0.769, 0.792, 0.829, 0.937]
# f1_16 = [0.786, 0.778, 0.767, 0.743, 0.686, 0.649, 0.579, 0.398]

# code_struct:
# recall_16 = [0.981, 0.965, 0.928, 0.821, 0.601, 0.502, 0.363, 0.200]
# precision_16 = [0.681, 0.702, 0.745, 0.803, 0.889, 0.918, 0.962, 0.996]
# f1_16 = [0.804, 0.813, 0.827, 0.812, 0.717, 0.649, 0.528, 0.333]

# ex_class_reference:
# recall_16 = [0.975, 0.953, 0.905, 0.775, 0.536, 0.432, 0.315, 0.186]
# precision_16 = [0.688, 0.719, 0.770, 0.828, 0.904, 0.932, 0.965, 0.997]
# f1_16 = [0.807, 0.820, 0.832, 0.801, 0.673, 0.590, 0.475, 0.314]

# global_call:
# recall_16 = [0.881, 0.819, 0.732, 0.613, 0.439, 0.369, 0.298, 0.225]
# precision_16 = [0.717, 0.737, 0.762, 0.810, 0.873, 0.895, 0.921, 0.947]
# f1_16 = [0.791, 0.776, 0.747, 0.698, 0.584, 0.523, 0.450, 0.363]

# halstead:
# recall_16 = [0.918, 0.883, 0.834, 0.765, 0.645, 0.583, 0.488, 0.298]
# precision_16 = [0.691, 0.701, 0.717, 0.738, 0.782, 0.808, 0.843, 0.921]
# f1_16 = [0.789, 0.782, 0.771, 0.751, 0.707, 0.677, 0.618, 0.450]

# literal:
# recall_16 = [0.515, 0.448, 0.383, 0.330, 0.275, 0.258, 0.236, 0.199]
# precision_16 = [0.855, 0.855, 0.864, 0.874, 0.892, 0.903, 0.926, 0.956]
# f1_16 = [0.642, 0.588, 0.531, 0.479, 0.420, 0.402, 0.376, 0.329]

# local_call:
# recall_16 = [0.961, 0.926, 0.867, 0.764, 0.563, 0.470, 0.365, 0.243]
# precision_16 = [0.657, 0.669, 0.692, 0.728, 0.796, 0.835, 0.875, 0.934]
# f1_16 = [0.781, 0.777, 0.770, 0.746, 0.659, 0.602, 0.515, 0.386]

# variables_methods:
# recall_16 = [0.931, 0.887, 0.808, 0.684, 0.472, 0.388, 0.303, 0.211]
# precision_16 = [0.686, 0.703, 0.726, 0.767, 0.823, 0.863, 0.911, 0.975]
# f1_16 = [0.790, 0.784, 0.765, 0.723, 0.600, 0.535, 0.455, 0.347]

# keywords:
# recall_16 = [0.617, 0.511, 0.409, 0.313, 0.233, 0.214, 0.193, 0.163]
# precision_16 = [0.820, 0.860, 0.888, 0.932, 0.983, 0.988, 0.991, 0.993]
# f1_16 = [0.704, 0.641, 0.560, 0.469, 0.377, 0.351, 0.323, 0.280]

# all metrics:
# recall_16 = [0.908, 0.845, 0.740, 0.584, 0.371, 0.298, 0.224, 0.159]
# precision_16 = [0.710, 0.739, 0.778, 0.842, 0.943, 0.972, 0.992, 0.999]
# f1_16 = [0.797, 0.789, 0.759, 0.690, 0.533, 0.456, 0.365, 0.275]

# both:
recall_16 = [0.646, 0.543, 0.431, 0.329, 0.235, 0.204, 0.178, 0.153]
precision_16 = [0.807, 0.849, 0.892, 0.943, 0.983, 0.991, 0.997, 0.999]
f1_16 = [0.718, 0.662, 0.581, 0.488, 0.379, 0.338, 0.302, 0.265]


# Average data of dimension 32
# complexity:
# recall_32 = [0.889, 0.842, 0.780, 0.688, 0.543, 0.467, 0.372, 0.225]
# precision_32 = [0.702, 0.716, 0.732, 0.749, 0.789, 0.815, 0.880, 0.964]
# f1_32 = [0.784, 0.774, 0.755, 0.717, 0.643, 0.594, 0.522, 0.364]

# code_struct:
# recall_32 = [0.983, 0.969, 0.936, 0.835, 0.615, 0.483, 0.363, 0.199]
# precision_32 = [0.679, 0.703, 0.747, 0.806, 0.888, 0.917, 0.955, 0.996]
# f1_32 = [0.803, 0.815, 0.831, 0.820, 0.726, 0.633, 0.526, 0.332]

# ex_class_reference:
# recall_32 = [0.983, 0.969, 0.937, 0.838, 0.621, 0.509, 0.366, 0.200]
# precision_32 = [0.680, 0.705, 0.749, 0.809, 0.880, 0.909, 0.946, 0.991]
# f1_32 = [0.804, 0.816, 0.833, 0.823, 0.728, 0.653, 0.528, 0.334]

# global_call:
# recall_32 = [0.885, 0.823, 0.737, 0.619, 0.441, 0.373, 0.299, 0.226]
# precision_32 = [0.715, 0.735, 0.761, 0.808, 0.872, 0.895, 0.920, 0.947]
# f1_32 = [0.791, 0.777, 0.748, 0.701, 0.586, 0.526, 0.452, 0.365]

# halstead:
# recall_32 = [0.919, 0.885, 0.837, 0.768, 0.653, 0.591, 0.499, 0.306]
# precision_32 = [0.690, 0.699, 0.714, 0.735, 0.776, 0.800, 0.836, 0.916]
# f1_32 = [0.788, 0.781, 0.771, 0.751, 0.709, 0.680, 0.625, 0.459]

# literal:
# recall_32 = [0.463, 0.414, 0.356, 0.311, 0.265, 0.249, 0.232, 0.198]
# precision_32 = [0.857, 0.860, 0.865, 0.870, 0.890, 0.904, 0.925, 0.955]
# f1_32 = [0.601, 0.559, 0.504, 0.458, 0.408, 0.391, 0.371, 0.328]

# local_call:
# recall_32 = [0.958, 0.921, 0.860, 0.752, 0.550, 0.462, 0.360, 0.241]
# precision_32 = [0.658, 0.671, 0.694, 0.730, 0.797, 0.838, 0.876, 0.934]
# f1_32 = [0.780, 0.776, 0.768, 0.741, 0.651, 0.595, 0.510, 0.384]

# variables_methods:
# recall_32 = [0.938, 0.896, 0.815, 0.682, 0.462, 0.378, 0.293, 0.203]
# precision_32 = [0.689, 0.707, 0.733, 0.777, 0.837, 0.878, 0.923, 0.978]
# f1_32 = [0.794, 0.790, 0.772, 0.726, 0.595, 0.528, 0.445, 0.336]

# keywords:
# recall_32 = [0.546, 0.435, 0.336, 0.251, 0.191, 0.179, 0.168, 0.151]
# precision_32 = [0.843, 0.892, 0.934, 0.974, 0.991, 0.993, 0.996, 0.998]
# f1_32 = [0.663, 0.585, 0.494, 0.399, 0.320, 0.303, 0.287, 0.262]

# all metrics:
# recall_32 = [0.933, 0.888, 0.813, 0.678, 0.440, 0.345, 0.247, 0.164]
# precision_32 = [0.696, 0.721, 0.761, 0.818, 0.891, 0.927, 0.972, 0.997]
# f1_32 = [0.797, 0.796, 0.786, 0.741, 0.589, 0.502, 0.394, 0.282]

# both:
recall_32 = [0.525, 0.430, 0.340, 0.255, 0.195, 0.179, 0.166, 0.146]
precision_32 = [0.839, 0.889, 0.938, 0.976, 0.992, 0.996, 0.998, 0.999]
f1_32 = [0.646, 0.580, 0.500, 0.404, 0.326, 0.303, 0.285, 0.255]


# Average data of dimension 64
# complexity:
# recall_64 = [0.886, 0.836, 0.770, 0.675, 0.528, 0.454, 0.36, 0.220]
# precision_64 = [0.703, 0.716, 0.731, 0.750, 0.791, 0.816, 0.869, 0.965]
# f1_64 = [0.784, 0.772, 0.750, 0.710, 0.633, 0.583, 0.509, 0.359]

# code_struct:
# recall_64 = [0.982, 0.967, 0.933, 0.831, 0.599, 0.483, 0.341, 0.191]
# precision_64 = [0.676, 0.700, 0.748, 0.805, 0.883, 0.917, 0.961, 0.995]
# f1_64 = [0.801, 0.812, 0.830, 0.818, 0.714, 0.633, 0.503, 0.321]

# ex_class_reference:
# recall_64 = [0.980, 0.965, 0.934, 0.840, 0.619, 0.503, 0.354, 0.192]
# precision_64 = [0.677, 0.701, 0.748, 0.800, 0.880, 0.909, 0.954, 0.997]
# f1_64 = [0.800, 0.812, 0.831, 0.820, 0.727, 0.647, 0.517, 0.321]

# global_call:
# recall_64 = [0.900, 0.843, 0.759, 0.644, 0.464, 0.387, 0.310, 0.228]
# precision_64 = [0.710, 0.730, 0.754, 0.798, 0.864, 0.892, 0.919, 0.946]
# f1_64 = [0.793, 0.783, 0.756, 0.713, 0.604, 0.540, 0.464, 0.367]

# halstead:
# recall_64 = [0.921, 0.885, 0.837, 0.767, 0.649, 0.587, 0.493, 0.301]
# precision_64 = [0.690, 0.700, 0.715, 0.737, 0.782, 0.806, 0.842, 0.921]
# f1_64 = [0.789, 0.781, 0.771, 0.752, 0.709, 0.680, 0.622, 0.454]

# literal:
# recall_64 = [0.469, 0.417, 0.360, 0.312, 0.265, 0.249, 0.232, 0.198]
# precision_64 = [0.855, 0.857, 0.864, 0.871, 0.889, 0.904, 0.924, 0.955]
# f1_64 = [0.606, 0.561, 0.508, 0.459, 0.409, 0.390, 0.371, 0.327]

# local_call:
# recall_64 = [0.964, 0.930, 0.872, 0.770, 0.571, 0.478, 0.369, 0.245]
# precision_64 = [0.655, 0.668, 0.690, 0.726, 0.792, 0.831, 0.873, 0.934]
# f1_64 = [0.780, 0.778, 0.770, 0.747, 0.664, 0.607, 0.519, 0.388]

# variables_methods:
# recall_64 = [0.957, 0.927, 0.870, 0.752, 0.524, 0.426, 0.321, 0.212]
# precision_64 = [0.681, 0.696, 0.718, 0.760, 0.825, 0.865, 0.913, 0.972]
# f1_64 = [0.796, 0.795, 0.787, 0.756, 0.641, 0.571, 0.475, 0.347]

# keywords:
# recall_64 = [0.457, 0.365, 0.288, 0.227, 0.185, 0.176, 0.166, 0.150]
# precision_64 = [0.917, 0.947, 0.971, 0.988, 0.993, 0.995, 0.997, 0.998]
# f1_64 = [0.610, 0.527, 0.445, 0.369, 0.311, 0.300, 0.284, 0.261]

# all metrics:
# recall_64 = [0.869, 0.782, 0.645, 0.464, 0.280, 0.231, 0.188, 0.153]
# precision_64 = [0.743, 0.787, 0.835, 0.879, 0.947, 0.973, 0.992, 1]
# f1_64 = [0.801, 0.785, 0.728, 0.608, 0.432, 0.374, 0.317, 0.226]

# both:
recall_64 = [0.469, 0.373, 0.291, 0.228, 0.179, 0.168, 0.159, 0.142]
precision_64 = [0.886, 0.930, 0.962, 0.986, 0.996, 0.997, 0.998, 0.999]
f1_64 = [0.613, 0.533, 0.447, 0.370, 0.303, 0.287, 0.274, 0.248]


# Average data of dimension 128
# complexity:
# recall_128 = [0.903, 0.857, 0.792, 0.696, 0.541, 0.465, 0.365, 0.219]
# precision_128 = [0.694, 0.705, 0.723, 0.744, 0.807, 0.849, 0.905, 0.969]
# f1_128 = [0.785, 0.774, 0.756, 0.719, 0.648, 0.601, 0.520, 0.357]

# code_struct:
# recall_128 = [0.962, 0.933, 0.876, 0.808, 0.537, 0.435, 0.319, 0.187]
# precision_128 = [0.684, 0.717, 0.760, 0.808, 0.886, 0.921, 0.962, 0.998]
# f1_128 = [0.799, 0.811, 0.814, 0.786, 0.669, 0.591, 0.479, 0.316]

# ex_class_reference:
# recall_128 = [0.962, 0.934, 0.878, 0.769, 0.544, 0.441, 0.324, 0.190]
# precision_128 = [0.683, 0.716, 0.758, 0.805, 0.882, 0.918, 0.960, 0.997]
# f1_128 = [0.799, 0.811, 0.814, 0.787, 0.673, 0.596, 0.484, 0.319]

# global_call:
# recall_128 = [0.903, 0.847, 0.765, 0.650, 0.468, 0.390, 0.313, 0.228]
# precision_128 = [0.708, 0.729, 0.753, 0.796, 0.861, 0.890, 0.918, 0.946]
# f1_128 = [0.793, 0.783, 0.759, 0.716, 0.607, 0.543, 0.467, 0.368]

# halstead:
# recall_128 = [0.919, 0.884, 0.836, 0.767, 0.650, 0.586, 0.492, 0.301]
# precision_128 = [0.690, 0.699, 0.715, 0.737, 0.782, 0.806, 0.841, 0.920]
# f1_128 = [0.788, 0.781, 0.771, 0.752, 0.709, 0.679, 0.621, 0.453]

# literal:
# recall_128 = [0.472, 0.413, 0.359, 0.311, 0.265, 0.249, 0.232, 0.197]
# precision_128 = [0.855, 0.856, 0.864, 0.870, 0.889, 0.904, 0.925, 0.954]
# f1_128 = [0.608, 0.558, 0.507, 0.459, 0.408, 0.391, 0.371, 0.327]

# local_call:
# recall_128 = [0.962, 0.927, 0.869, 0.767, 0.565, 0.475, 0.367, 0.243]
# precision_128 = [0.656, 0.668, 0.691, 0.727, 0.792, 0.832, 0.874, 0.933]
# f1_128 = [0.780, 0.777, 0.770, 0.746, 0.659, 0.605, 0.517, 0.386]

# variables_methods:
# recall_128 = [0.948, 0.912, 0.843, 0.710, 0.484, 0.395, 0.301, 0.204]
# precision_128 = [0.687, 0.704, 0.730, 0.773, 0.837, 0.878, 0.923, 0.975]
# f1_128 = [0.797, 0.795, 0.782, 0.740, 0.613, 0.545, 0.454, 0.338]

# keywords:
# recall_128 = [0.421, 0.338, 0.273, 0.219, 0.183, 0.174, 0.165, 0.150]
# precision_128 = [0.896, 0.930, 0.959, 0.983, 0.991, 0.993, 0.996, 0.998]
# f1_128 = [0.573, 0.496, 0.425, 0.358, 0.309, 0.296, 0.283, 0.260]

# all metrics:
# recall_128 = [0.856, 0.763, 0.631, 0.458, 0.276, 0.228, 0.188, 0.152]
# precision_128 = [0.744, 0.780, 0.826, 0.883, 0.959, 0.980, 0.995, 1]
# f1_128 = [0.796, 0.771, 0.715, 0.603, 0.428, 0.370, 0.317, 0.265]

# both:
recall_128 = [0.432, 0.344, 0.270, 0.214, 0.175, 0.166, 0.158, 0.142]
precision_128 = [0.902, 0.937, 0.969, 0.989, 0.996, 0.997, 0.999, 0.999]
f1_128 = [0.584, 0.503, 0.422, 0.352, 0.298, 0.285, 0.272, 0.248]


# Create subgraph
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
fig.suptitle("Performance Metrics Across Dimensions", fontsize=16)

# Dimension 16
axes[0, 0].plot(cosine_values, recall_16, label='Recall', marker='o')
axes[0, 0].plot(cosine_values, precision_16, label='Precision', marker='o')
axes[0, 0].plot(cosine_values, f1_16, label='F1-Score', marker='o')
axes[0, 0].set_title('Dimension = 16')
axes[0, 0].set_xlabel('Cosine Threshold')
axes[0, 0].set_ylabel('Metric Value')

# Dimension 32
axes[0, 1].plot(cosine_values, recall_32, label='Recall', marker='o')
axes[0, 1].plot(cosine_values, precision_32, label='Precision', marker='o')
axes[0, 1].plot(cosine_values, f1_32, label='F1-Score', marker='o')
axes[0, 1].set_title('Dimension = 32')
axes[0, 1].set_xlabel('Cosine Threshold')
axes[0, 1].set_ylabel('Metric Value')

# Dimension 64
axes[1, 0].plot(cosine_values, recall_64, label='Recall', marker='o')
axes[1, 0].plot(cosine_values, precision_64, label='Precision', marker='o')
axes[1, 0].plot(cosine_values, f1_64, label='F1-Score', marker='o')
axes[1, 0].set_title('Dimension = 64')
axes[1, 0].set_xlabel('Cosine Threshold')
axes[1, 0].set_ylabel('Metric Value')

# Dimension 128
axes[1, 1].plot(cosine_values, recall_128, label='Recall', marker='o')
axes[1, 1].plot(cosine_values, precision_128, label='Precision', marker='o')
axes[1, 1].plot(cosine_values, f1_128, label='F1-Score', marker='o')
axes[1, 1].set_title('Dimension = 128')
axes[1, 1].set_xlabel('Cosine Threshold')
axes[1, 1].set_ylabel('Metric Value')

# Display legend
for ax in axes.flat:
    ax.legend()

# Adjust layout
plt.tight_layout(rect=[0, 0.03, 1, 0.95])

plt.savefig('./figures/performance_both.png', dpi=300, bbox_inches='tight')

plt.show()
